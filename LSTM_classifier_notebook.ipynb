{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Импорт нужных библиотек."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import codecs\n",
    "import numpy as np\n",
    "import re\n",
    "import collections\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'C:\\\\Users\\\\TOPKEK\\\\Documents\\\\Python_Directory\\\\aclImdb'\n",
    "train_texts = []\n",
    "train_labels = []\n",
    "for category in ['pos', 'neg']:\n",
    "    train_data_path = os.path.join(data_path, 'train', category)\n",
    "    for fname in sorted(os.listdir(train_data_path)):\n",
    "        if fname.endswith('.txt'):\n",
    "            with codecs.open(os.path.join(train_data_path, fname), 'r', 'utf_8_sig') as f:\n",
    "                train_texts.append(f.read())\n",
    "            train_labels.append(0 if category == 'neg' else 1)\n",
    "\n",
    "test_texts = []\n",
    "test_labels = []\n",
    "for category in ['pos', 'neg']:\n",
    "    test_data_path = os.path.join(data_path, 'test', category)\n",
    "    for fname in sorted(os.listdir(test_data_path)):\n",
    "        if fname.endswith('.txt'):\n",
    "            with codecs.open(os.path.join(test_data_path, fname), 'r', 'utf_8_sig') as f:\n",
    "                test_texts.append(f.read())\n",
    "            test_labels.append(0 if category == 'neg' else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Наибольшая длина отзыва 13704\n",
      "Средняя длина отзыва 1325\n"
     ]
    }
   ],
   "source": [
    "print('Наибольшая длина отзыва', max([len(rewiew) for rewiew in train_texts]))\n",
    "print('Средняя длина отзыва', int(np.mean([len(rewiew) for rewiew in train_texts])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обработка данных\n",
    "\n",
    "## Токенизатор\n",
    "\n",
    "Для обработки текста мной был написан такой токенизатор. Он заменяет некоторые не имеющие для нас смысла символы в тексте на пробелы, также делает ещё пару упрощений. Ну и, соответсвенно, делает саму токенизацию. Также поставил минимальный размер токена равный трём, чтобы учитывать важные для классификации слова как \"not\", \"bad\", и так далее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "_patterns = [r'\\\"',\n",
    "             r'<br /><br />',\n",
    "             r'\\;',\n",
    "             r'\\:',\n",
    "             r'\\s+',\n",
    "             r'\\(',\n",
    "             r'\\)']\n",
    "\n",
    "_replacements = ['',\n",
    "                 ' ',\n",
    "                 ' ',\n",
    "                 ' ',\n",
    "                 ' ',\n",
    "                '',\n",
    "                '']\n",
    "_patterns_dict = list((re.compile(p), r) for p, r in zip(_patterns, _replacements))\n",
    "\n",
    "def normalize(line):\n",
    "    line = line.lower()\n",
    "    for pattern_re, replaced_str in _patterns_dict:\n",
    "        line = pattern_re.sub(replaced_str, line)\n",
    "    return line\n",
    "\n",
    "TOKEN_RE = re.compile(r'[a-z]+|\\d+[.,]\\d+|\\d+')\n",
    "\n",
    "def tokenize(txt, min_token_size = 3):\n",
    "    txt = normalize(txt)\n",
    "    all_tokens = TOKEN_RE.findall(txt)\n",
    "    return [token for token in all_tokens if len(token) >= min_token_size]\n",
    "\n",
    "def tokenize_corpus(texts, tokenizer=tokenize, **tokenizer_kwargs):\n",
    "    return [tokenizer(text, **tokenizer_kwargs) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = train_texts + test_texts\n",
    "train_labels = train_labels + test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "\n",
    "# Функция, собирающая в себе нормализацию и токенизацию всего корпуса одновременно\n",
    "def preprocess(text):\n",
    "    text = [normalize(texti) for texti in text]\n",
    "    all_reviews = tokenize_corpus(text, min_token_size = 3)\n",
    "    text = \" \".join(text)\n",
    "    all_words = tokenize(text, min_token_size = 3)\n",
    "    \n",
    "    return all_reviews, all_words\n",
    "\n",
    "\n",
    "all_reviews, all_words = preprocess(train_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Построение словаря\n",
    "\n",
    "Также необходимо занумеровать токены. Для этого строятся два обратных словаря, сопоставляющие друг другу номер токена и сам токен."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "word_counts = Counter(all_words)\n",
    "word_list = sorted(word_counts, key = word_counts.get, reverse = True)\n",
    "vocab_to_int = {word:idx+1 for idx, word in enumerate(word_list)}\n",
    "int_to_vocab = {idx:word for word, idx in vocab_to_int.items()}\n",
    "encoded_reviews = [[vocab_to_int[word] for word in review] for review in all_reviews]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метки были уже закодированы на процессе считывания данных, так что их оставляем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_labels = train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_labels = np.array( [label for idx, label in enumerate(encoded_labels) if len(encoded_reviews[idx]) > 0] )\n",
    "encoded_reviews = [review for review in encoded_reviews if len(review) > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция ниже дополняет нулями закодированные тексты так, чтобы они все имели одинаковую длину."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_text(encoded_reviews, seq_length):\n",
    "    \n",
    "    reviews = []\n",
    "    \n",
    "    for review in encoded_reviews:\n",
    "        if len(review) >= seq_length:\n",
    "            reviews.append(review[:seq_length])\n",
    "        else:\n",
    "            reviews.append([0]*(seq_length-len(review)) + review)\n",
    "        \n",
    "    return np.array(reviews)\n",
    "\n",
    "\n",
    "padded_reviews = pad_text(encoded_reviews, seq_length = 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Простенькая функция, перемешивающая тексты и метки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unison_shuffled_copies(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]\n",
    "padded_reviews, encoded_labels = unison_shuffled_copies(padded_reviews, encoded_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример закодированного текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,   345,    58,     3,     6,\n",
       "          42,  1599,   308,   161,     1,   800,    45,   141,   984,\n",
       "           2,     5,  9871,     7,   188,     6,    61,    51,   318,\n",
       "         104,   257,   951,  3648,  2616,     5,   596,     3,     7,\n",
       "           1,     6,     4,     4,   268,   106,  1421,   297,     1,\n",
       "        6249,     1,  1241,  1221,     1,    41, 27439,    96,     2,\n",
       "           1,   233,   277,   193,   999,     1,    89,     1,     6,\n",
       "           5,    34,  1570,    17,    17,   166,     7,    55,   286,\n",
       "           9,     1,     6,    71,    15,  6420,    49,  1917,     8,\n",
       "         193,    33,   634,     6,   199, 28241,   242,   280,   125,\n",
       "         589,     7,    81,   578,   116,     9,    61,    94,     4,\n",
       "         339,   362])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_reviews[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ниже мы разбиваем __все__ наши данные на три сета: тренировочный, валидационный и тестовый."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.8\n",
    "valid_ratio = (1 - train_ratio)/2\n",
    "total = padded_reviews.shape[0]\n",
    "train_cutoff = int(total * train_ratio)\n",
    "valid_cutoff = int(total * (1 - valid_ratio))\n",
    "\n",
    "train_x, train_y = padded_reviews[:train_cutoff], encoded_labels[:train_cutoff]\n",
    "valid_x, valid_y = padded_reviews[train_cutoff : valid_cutoff], encoded_labels[train_cutoff : valid_cutoff]\n",
    "test_x, test_y = padded_reviews[valid_cutoff:], encoded_labels[valid_cutoff:]\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "#---------------\n",
    "train_x = torch.Tensor(train_x).long()\n",
    "train_y = torch.Tensor(train_y).long()\n",
    "valid_x = torch.Tensor(valid_x).long()\n",
    "valid_y = torch.Tensor(valid_y).long()\n",
    "test_x = torch.Tensor(test_x).long()\n",
    "test_y = torch.Tensor(test_y).long()\n",
    "#---------------\n",
    "train_data = TensorDataset(train_x, train_y)\n",
    "valid_data = TensorDataset(valid_x, valid_y)\n",
    "test_data = TensorDataset(test_x, test_y)\n",
    "\n",
    "batch_size = 50\n",
    "train_loader = DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "valid_loader = DataLoader(valid_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = DataLoader(test_data, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Построение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_vocab, n_embed, n_hidden, n_output, n_layers, drop_p = 0.5):\n",
    "        super().__init__()\n",
    "        self.n_vocab = n_vocab     # количесво уникальных слов\n",
    "        self.n_layers = n_layers   # слои\n",
    "        self.n_hidden = n_hidden   # нейронов в слоях\n",
    "        \n",
    "        self.embedding = nn.Embedding(n_vocab + 1, n_embed)\n",
    "        self.lstm = nn.LSTM(n_embed, n_hidden, n_layers, batch_first = True, dropout = drop_p)\n",
    "        self.dropout = nn.Dropout(drop_p)\n",
    "        self.fc = nn.Linear(n_hidden, n_output)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "    def forward (self, input_words):\n",
    "                                 # Размерности матриц   :  (batch_size, seq_length)\n",
    "        embedded_words = self.embedding(input_words)    # (batch_size, seq_length, n_embed)\n",
    "        lstm_out, h = self.lstm(embedded_words)         # (batch_size, seq_length, n_hidden)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.n_hidden) # (batch_size*seq_length, n_hidden)\n",
    "        fc_out = self.fc(lstm_out)                      # (batch_size*seq_length, n_output)\n",
    "        sigmoid_out = self.sigmoid(fc_out)              # (batch_size*seq_length, n_output)\n",
    "        sigmoid_out = sigmoid_out.view(batch_size, -1)  # (batch_size, seq_length*n_output)\n",
    "        \n",
    "        # extract the output of ONLY the LAST output of the LAST element of the sequence\n",
    "        # Искомое число - лишь последний выход последнего элемента рекуррентной последовательности\n",
    "        sigmoid_last = sigmoid_out[:, -1]               # (batch_size, 1)\n",
    "        \n",
    "        return sigmoid_last, h\n",
    "    \n",
    "    \n",
    "    def init_hidden (self, batch_size):  # initialize hidden weights (h,c) to 0\n",
    "        \n",
    "        device = 'cpu'\n",
    "        weights = next(self.parameters()).data\n",
    "        h = (weights.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device),\n",
    "             weights.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device))\n",
    "        \n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Настройка параметров. Из-за не самого большого количества как видео, так и оперативной памяти в моём расположении сейчас, сеть будет очень скромная."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vocab = len(vocab_to_int)\n",
    "n_embed = 32\n",
    "n_hidden = 8\n",
    "n_output = 1   \n",
    "n_layers = 2\n",
    "\n",
    "net = SentimentLSTM(n_vocab, n_embed, n_hidden, n_output, n_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr = 0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TOPKEK\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10 Step: 100 Training Loss: 0.6938 Validation Loss: 0.6974\n",
      "Epoch: 1/10 Step: 200 Training Loss: 0.6912 Validation Loss: 0.6929\n",
      "Epoch: 1/10 Step: 300 Training Loss: 0.6890 Validation Loss: 0.6938\n",
      "Epoch: 1/10 Step: 400 Training Loss: 0.6829 Validation Loss: 0.6879\n",
      "Epoch: 1/10 Step: 500 Training Loss: 0.6687 Validation Loss: 0.6753\n",
      "Epoch: 1/10 Step: 600 Training Loss: 0.6808 Validation Loss: 0.6678\n",
      "Epoch: 1/10 Step: 700 Training Loss: 0.6288 Validation Loss: 0.5631\n",
      "Epoch: 1/10 Step: 800 Training Loss: 0.6152 Validation Loss: 0.5583\n",
      "Epoch: 2/10 Step: 900 Training Loss: 0.6278 Validation Loss: 0.5878\n",
      "Epoch: 2/10 Step: 1000 Training Loss: 0.5969 Validation Loss: 0.5670\n",
      "Epoch: 2/10 Step: 1100 Training Loss: 0.5802 Validation Loss: 0.6017\n",
      "Epoch: 2/10 Step: 1200 Training Loss: 0.5283 Validation Loss: 0.4994\n",
      "Epoch: 2/10 Step: 1300 Training Loss: 0.6690 Validation Loss: 0.6441\n",
      "Epoch: 2/10 Step: 1400 Training Loss: 0.6589 Validation Loss: 0.6270\n",
      "Epoch: 2/10 Step: 1500 Training Loss: 0.5292 Validation Loss: 0.5795\n",
      "Epoch: 2/10 Step: 1600 Training Loss: 0.4392 Validation Loss: 0.4592\n",
      "Epoch: 3/10 Step: 1700 Training Loss: 0.4430 Validation Loss: 0.4103\n",
      "Epoch: 3/10 Step: 1800 Training Loss: 0.4440 Validation Loss: 0.3844\n",
      "Epoch: 3/10 Step: 1900 Training Loss: 0.4862 Validation Loss: 0.4619\n",
      "Epoch: 3/10 Step: 2000 Training Loss: 0.4796 Validation Loss: 0.5250\n",
      "Epoch: 3/10 Step: 2100 Training Loss: 0.3980 Validation Loss: 0.4399\n",
      "Epoch: 3/10 Step: 2200 Training Loss: 0.4643 Validation Loss: 0.4215\n",
      "Epoch: 3/10 Step: 2300 Training Loss: 0.4850 Validation Loss: 0.5065\n",
      "Epoch: 3/10 Step: 2400 Training Loss: 0.3564 Validation Loss: 0.3488\n",
      "Epoch: 4/10 Step: 2500 Training Loss: 0.3259 Validation Loss: 0.2870\n",
      "Epoch: 4/10 Step: 2600 Training Loss: 0.3605 Validation Loss: 0.3117\n",
      "Epoch: 4/10 Step: 2700 Training Loss: 0.3143 Validation Loss: 0.2654\n",
      "Epoch: 4/10 Step: 2800 Training Loss: 0.4024 Validation Loss: 0.3297\n",
      "Epoch: 4/10 Step: 2900 Training Loss: 0.3779 Validation Loss: 0.2787\n",
      "Epoch: 4/10 Step: 3000 Training Loss: 0.4267 Validation Loss: 0.4390\n",
      "Epoch: 4/10 Step: 3100 Training Loss: 0.2923 Validation Loss: 0.2814\n",
      "Epoch: 4/10 Step: 3200 Training Loss: 0.7312 Validation Loss: 0.5524\n",
      "Epoch: 5/10 Step: 3300 Training Loss: 0.2450 Validation Loss: 0.2218\n",
      "Epoch: 5/10 Step: 3400 Training Loss: 0.4377 Validation Loss: 0.3915\n",
      "Epoch: 5/10 Step: 3500 Training Loss: 0.4215 Validation Loss: 0.4191\n",
      "Epoch: 5/10 Step: 3600 Training Loss: 0.3923 Validation Loss: 0.2663\n",
      "Epoch: 5/10 Step: 3700 Training Loss: 0.2746 Validation Loss: 0.2996\n",
      "Epoch: 5/10 Step: 3800 Training Loss: 0.4066 Validation Loss: 0.3182\n",
      "Epoch: 5/10 Step: 3900 Training Loss: 0.4856 Validation Loss: 0.3712\n",
      "Epoch: 5/10 Step: 4000 Training Loss: 0.4658 Validation Loss: 0.4133\n",
      "Epoch: 6/10 Step: 4100 Training Loss: 0.1728 Validation Loss: 0.1580\n",
      "Epoch: 6/10 Step: 4200 Training Loss: 0.1400 Validation Loss: 0.1505\n",
      "Epoch: 6/10 Step: 4300 Training Loss: 0.3052 Validation Loss: 0.2798\n",
      "Epoch: 6/10 Step: 4400 Training Loss: 0.1809 Validation Loss: 0.0912\n",
      "Epoch: 6/10 Step: 4500 Training Loss: 0.2986 Validation Loss: 0.2496\n",
      "Epoch: 6/10 Step: 4600 Training Loss: 0.2742 Validation Loss: 0.2028\n",
      "Epoch: 6/10 Step: 4700 Training Loss: 0.2454 Validation Loss: 0.1708\n",
      "Epoch: 6/10 Step: 4800 Training Loss: 0.0938 Validation Loss: 0.0429\n",
      "Epoch: 7/10 Step: 4900 Training Loss: 0.4303 Validation Loss: 0.4310\n",
      "Epoch: 7/10 Step: 5000 Training Loss: 0.1929 Validation Loss: 0.1724\n",
      "Epoch: 7/10 Step: 5100 Training Loss: 0.1800 Validation Loss: 0.1642\n",
      "Epoch: 7/10 Step: 5200 Training Loss: 0.2024 Validation Loss: 0.1432\n",
      "Epoch: 7/10 Step: 5300 Training Loss: 0.3898 Validation Loss: 0.2562\n",
      "Epoch: 7/10 Step: 5400 Training Loss: 0.1835 Validation Loss: 0.1323\n",
      "Epoch: 7/10 Step: 5500 Training Loss: 0.2045 Validation Loss: 0.1936\n",
      "Epoch: 7/10 Step: 5600 Training Loss: 0.1520 Validation Loss: 0.1546\n",
      "Epoch: 8/10 Step: 5700 Training Loss: 0.1007 Validation Loss: 0.0493\n",
      "Epoch: 8/10 Step: 5800 Training Loss: 0.1993 Validation Loss: 0.1196\n",
      "Epoch: 8/10 Step: 5900 Training Loss: 0.2066 Validation Loss: 0.0952\n",
      "Epoch: 8/10 Step: 6000 Training Loss: 0.2525 Validation Loss: 0.2832\n",
      "Epoch: 8/10 Step: 6100 Training Loss: 0.1274 Validation Loss: 0.1100\n",
      "Epoch: 8/10 Step: 6200 Training Loss: 0.1234 Validation Loss: 0.0504\n",
      "Epoch: 8/10 Step: 6300 Training Loss: 0.2049 Validation Loss: 0.1614\n",
      "Epoch: 8/10 Step: 6400 Training Loss: 0.3776 Validation Loss: 0.3030\n",
      "Epoch: 9/10 Step: 6500 Training Loss: 0.0929 Validation Loss: 0.0870\n",
      "Epoch: 9/10 Step: 6600 Training Loss: 0.1524 Validation Loss: 0.1202\n",
      "Epoch: 9/10 Step: 6700 Training Loss: 0.2038 Validation Loss: 0.2276\n",
      "Epoch: 9/10 Step: 6800 Training Loss: 0.0680 Validation Loss: 0.0671\n",
      "Epoch: 9/10 Step: 6900 Training Loss: 0.1052 Validation Loss: 0.0491\n",
      "Epoch: 9/10 Step: 7000 Training Loss: 0.0532 Validation Loss: 0.0411\n",
      "Epoch: 9/10 Step: 7100 Training Loss: 0.1943 Validation Loss: 0.1356\n",
      "Epoch: 9/10 Step: 7200 Training Loss: 0.0437 Validation Loss: 0.0558\n",
      "Epoch: 10/10 Step: 7300 Training Loss: 0.2030 Validation Loss: 0.1920\n",
      "Epoch: 10/10 Step: 7400 Training Loss: 0.0349 Validation Loss: 0.0169\n",
      "Epoch: 10/10 Step: 7500 Training Loss: 0.0576 Validation Loss: 0.0209\n",
      "Epoch: 10/10 Step: 7600 Training Loss: 0.1251 Validation Loss: 0.1894\n",
      "Epoch: 10/10 Step: 7700 Training Loss: 0.1126 Validation Loss: 0.1372\n",
      "Epoch: 10/10 Step: 7800 Training Loss: 0.0558 Validation Loss: 0.0462\n",
      "Epoch: 10/10 Step: 7900 Training Loss: 0.2247 Validation Loss: 0.2045\n",
      "Epoch: 10/10 Step: 8000 Training Loss: 0.1495 Validation Loss: 0.1619\n"
     ]
    }
   ],
   "source": [
    "print_every = 100\n",
    "step = 0\n",
    "n_epochs = 10  \n",
    "clip = 5  # предотвращение градиентного скачка\n",
    "device = 'cpu'\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    h = net.init_hidden(batch_size)\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        step += 1\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        h = tuple([each.data for each in h])   \n",
    "        \n",
    "        net.zero_grad()\n",
    "        output, h = net(inputs)\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        # Наблюдение за процессом обучения.\n",
    "        if (step % print_every) == 0:            \n",
    "            net.eval()\n",
    "            valid_losses = []\n",
    "            v_h = net.init_hidden(batch_size)\n",
    "            \n",
    "            for v_inputs, v_labels in valid_loader:\n",
    "                v_inputs, v_labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "                v_h = tuple([each.data for each in v_h])\n",
    "                \n",
    "                v_output, v_h = net(v_inputs)\n",
    "                v_loss = criterion(v_output.squeeze(), v_labels.float())\n",
    "                valid_losses.append(v_loss.item())\n",
    "\n",
    "            print(\"Epoch: {}/{}\".format((epoch+1), n_epochs),\n",
    "                  \"Step: {}\".format(step),\n",
    "                  \"Training Loss: {:.4f}\".format(loss.item()),\n",
    "                  \"Validation Loss: {:.4f}\".format(np.mean(valid_losses)))\n",
    "            net.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Оценка качества модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.5193\n",
      "Test Accuracy: 0.85\n"
     ]
    }
   ],
   "source": [
    "net.eval()\n",
    "test_losses = []\n",
    "num_correct = 0\n",
    "test_h = net.init_hidden(batch_size)\n",
    "\n",
    "for inputs, labels in test_loader:\n",
    "    test_h = tuple([each.data for each in test_h])\n",
    "    test_output, test_h = net(inputs)\n",
    "    loss = criterion(test_output, labels.float())\n",
    "    test_losses.append(loss.item())\n",
    "    \n",
    "    preds = torch.round(test_output.squeeze())\n",
    "    correct_tensor = preds.eq(labels.float().view_as(preds))\n",
    "    correct = np.squeeze(correct_tensor.numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "    \n",
    "print(\"Test Loss: {:.4f}\".format(np.mean(test_losses)))\n",
    "print(\"Test Accuracy: {:.2f}\".format(num_correct/len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37367857 3.0\n"
     ]
    }
   ],
   "source": [
    "def predict(net, review, seq_length = 200):\n",
    "    device = 'cpu' \n",
    "    \n",
    "    words, _ = preprocess([review])\n",
    "    #print(words)\n",
    "    encoded_words = [vocab_to_int[word] for word in words[0]]\n",
    "    padded_words = pad_text([encoded_words], seq_length)\n",
    "    padded_words = torch.from_numpy(padded_words).to(device)\n",
    "    \n",
    "    if(len(padded_words) == 0):\n",
    "        \"Your review must contain at least 1 word!\"\n",
    "        return None\n",
    "    \n",
    "    net.eval()\n",
    "    h = net.init_hidden(1)\n",
    "    padded_words = torch.Tensor(padded_words.float()).long()\n",
    "    output, h = net(padded_words)\n",
    "    pred = output.squeeze()\n",
    "    \n",
    "    return pred\n",
    "\n",
    "def rating_pred(out):\n",
    "    if out > .5:\n",
    "        return round(7 + 6 * (out - 0.5))\n",
    "    else:\n",
    "        return round(4 - 6 * (-out + 0.5))\n",
    "    \n",
    "    \n",
    "review = \"I think that this movie can be better. But, unfortunately, it is not.\"\n",
    "\n",
    "print(predict(net, review)[-1].detach().numpy(), rating_pred(predict(net, review1)[-1].detach().numpy())) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
